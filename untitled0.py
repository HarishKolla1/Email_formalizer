# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTv4p6m5JI8Gr13WpHCwS4RyMh6SYpoE
"""

!pip install -q transformers datasets evaluate sacrebleu

import pandas as pd
from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import evaluate
import numpy as np

df = pd.read_csv("/content/email_formalizer_dataset_5000.csv")[["input_text", "target_text"]].dropna()
df = df.rename(columns={"input_text": "informal_text", "target_text": "formal_text"})

df = df.sample(n=5000, random_state=42) if len(df) > 5000 else df

dataset = Dataset.from_pandas(df)
dataset = dataset.train_test_split(test_size=0.1)
train_ds = dataset["train"]
test_ds = dataset["test"]

tokenizer = T5Tokenizer.from_pretrained("t5-small")

def preprocess_function(examples):
    inputs = ["formalize: " + text for text in examples["informal_text"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(examples["formal_text"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_ds.map(preprocess_function, batched=True)
tokenized_test = test_ds.map(preprocess_function, batched=True)

model = T5ForConditionalGeneration.from_pretrained("t5-small")

bleu = evaluate.load("sacrebleu")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])
    return {"bleu": result["score"]}

training_args = TrainingArguments(
    output_dir="./t5-email-formalizer",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=1,
    save_steps=500
)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

model.save_pretrained("/content/t5-email-formalizer")
tokenizer.save_pretrained("/content/t5-email-formalizer")

from google.colab import files
!zip -r t5-email-formalizer.zip t5-email-formalizer
files.download("t5-email-formalizer.zip")

import torch

def formalize_email(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    input_ids = tokenizer("formalize: " + text, return_tensors="pt", padding=True, truncation=True).input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)


# Sample Inputs
examples = [
    "pls find the doc attached",
    "sorry I can't make it to the meeting",
    "hey, send me the file asap!",
    "can't attend class today, not feeling well"
]

for text in examples:
    print(f"Informal: {text}")
    print(f"Formal  : {formalize_email(text)}\n")

!pip install streamlit pyngrok